\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{xcolor}
\usepackage[colorlinks, linkcolor=purple, citecolor=blue!75!black, urlcolor=purple]{hyperref}

\usepackage[noabbrev,capitalise]{cleveref}

\usepackage{libertine}
\usepackage[libertine,vvarbb]{newtxmath}
\usepackage[scaled=0.96]{zi4}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{claim}[theorem]{Claim}

\newcommand*{\defn}[1]{\textbf{#1}}
\newcommand*{\N}[0]{\mathbb{N}}
\newcommand*{\R}[0]{\mathbb{R}}
\newcommand*{\E}[0]{\mathbf{E}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand*{\pheq}{\mathrel{\phantom{=}}} % invisible = for align mode

\def\[#1\]{\begin{align*}#1\end{align*}}

\title{Notes on Convexity Inequalities}
\author{Lily Chung}
\date{}

\newlist{thmenum}{enumerate}{1}
\setlist[thmenum]{label=\alph*), ref=\thetheorem~(\alph*)}
\crefalias{thmenumi}{theorem} 

\begin{document}
\maketitle

These are some notes on inequalities arising from convexity---currently unfinished/wrong/disorganized.

In the following $(S, \Lambda, \mu)$ will be a $\sigma$-finite measure space.
We will mostly be interested in the cases where $S$ is a probability space ($\mu(S) = 1$) or where $S = \N$ and $\mu$ is counting measure.

\begin{definition}
  Let $T$ be a convex subset of $\R$.
  A real-valued function $f : T \to \R$ is \defn{convex} if for any $a, b \in T$ and any $t \in [0, 1]$ we have $f(ta + (1 - t)b) \le tf(a) + (1 - t)f(b)$.
\end{definition}

It can be checked that if $f$ is convex on $[a, b]$ then it is continuous on $(a, b)$, left-differentiable on $(a, b]$, and right-differentiable on $[a, b)$, the derivatives are increasing, and $f(y) - f(x) = \int_x^y f'(t)\,dt$ where $x, y \in (a, b)$ and $f'$ is either derivative of $f$.

%% Let $h: S \to \R_{\ge 0}$ be a nonnegative measurable function.
%% We define functions $m_h : \R_{\ge 0} \to [0, \mu(S)]$ and $h^* : [0, \mu(S)] \to \R_{\ge 0}$ via
%% \[
%% m_h(t) &= \mu(\{x : h(x) > t\}) \\
%% h^*(u) &= \sup\{t : m_h(t) > u\}
%% \]

%% We can think of $h^*$ as a ``decreasing rearrangement'' of $h$; it is a decreasing measurable function on $[0, \mu(S)]$ with the same distribution as $h$.

\begin{definition}
  Let $f, g : S \to \R_{\ge 0}$ be nonnegative measurable functions such that $\int f\,d\mu = \int g\,d\mu < \infty$.
  We say $g$ \defn{majorizes} $f$ if any of the following equivalent conditions hold:
  \begin{itemize}
  \item $\int \max\{f - t, 0\}\,d\mu \le \int \max\{g - t, 0\}\,d\mu$ for every $t \ge 0$.
  \item $\int_t^\infty m_f(u)\,du \le \int_t^\infty m_g(u)\,du$ for all $t \ge 0$, where $m_h(u) = \mu(\{x : h(x) > u\})$.
  \item $\int_0^r f^*(\ell)\,d\ell \le \int_0^r g^*(\ell)\,d\ell$ for all $0 \le r < \mu(S)$, where $h^*(\ell) = \sup\{t : m_h(t) > \ell\}$.
  \end{itemize}
\end{definition}

We write $f \prec g$ to mean $g$ majorizes $f$.
We prove a technical lemma:

\begin{lemma}\label{lem:attain max}
  Suppose $f \prec g$, and let $b = \max\{\esssup f, \esssup g\}$.  Then $\mu(\{x : f(x) = b\}) \le \mu(\{x : g(x) = b\})$.
\end{lemma}
\begin{proof}
  If $\esssup f \ne \esssup g$ the result is trivial.
  For $h \in \{f, g\}$ and $\epsilon > 0$ we have
  \[
  \int \max\{h - (b - \epsilon), 0\}\,d\mu &\in \epsilon[\mu(h = b), \mu(h \ge b - \epsilon)] \\
  \frac{\int \max\{h - (b - \epsilon), 0\}\,d\mu}{\epsilon} &\in \mu(h = b) + [0, \mu(b - \epsilon \le h < b)] \\
  \lim_{\epsilon \to 0^+} \frac{\int \max\{h - (b - \epsilon), 0\}\,d\mu}{\epsilon} &= \mu(h = b)
  \]
  But $\frac{\int \max\{f - (b - \epsilon), 0\}\,d\mu}{\epsilon} \le \frac{\int \max\{g - (b - \epsilon), 0\}\,d\mu}{\epsilon}$ for all $\epsilon > 0$
  and so $\mu(f = b) \le \mu(g = b)$.
\end{proof}


\begin{theorem}[Theorem 2.1 in Joe 1987]
  \label{thm:majorize}
  Let $\phi$ be a convex function on an interval $T \subset \R$ containing 0
  such that $\phi(0) = 0$.
  Let $f \prec g$ where $f, g$ take values in $T$.
  Then \[\int (\phi \circ f)\,d\mu \le \int (\phi \circ g)\,d\mu\]
  provided that the integrals exist and are finite.
\end{theorem}
\begin{proof}
  First we consider the case where $\phi$ is continuous and $\phi'(0^+) > -\infty$.  In this case, we can approximate $\phi$ by an increasing sequence of finite sums of the form $\phi_n(u) = \phi'(0^+)u + \sum_i a_i \max\{u - b_i, 0\}$ where $a_i, b_i \ge 0$.  Linearity gives us $\int (\phi_n \circ f)\,d\mu \le \int (\phi_n \circ g)\,d\mu$ and the result follows from the monotone convergence theorem.  In this case we don't need the condition that the integrals are finite: they always exist and the result holds even if one or both is $\infty$.

  For the next case we allow $\phi'(0^+) = -\infty$, and assume $\phi$ is continuous at its right endpoint but not necessarily its left.
  We approximate $\phi$ by a sequence of continuous convex functions
  \[\psi_n(u) = 
  \begin{cases}
    n\phi(n^{-1})u & 0 \le u < n^{-1}\\
    \phi(u) & u \ge n^{-1}
  \end{cases}\]
  where each $\psi_n'(0^+) > -\infty$.
  By the first case we have $\int (\psi_n \circ f)\,d\mu \le \int (\psi_n \circ g)\,d\mu$ and we can finish using the dominated convergence theorem since $|\psi_n| \le |\phi|$.

  Finally we handle the case where $\phi$ is discontinuous at its right endpoint, which we call $b$.  We can write $\phi(u) = \widetilde\phi(u) + c[u = b]$ for some $c > 0$ and some $\widetilde\phi$ which satisfies the conditions of the previous case.
  The result then follows from Lemma~\ref{lem:attain max}.
\end{proof}

Note that if $\mu(S) < \infty$ we can work with a general interval $[a, b]$ and dispense with the assumption $\phi(0) = 0$,
by considering the functions \[\phi_a(u) &= \phi(u + a) - \phi(a) \\ f_a(x) &= f(x) - a \\ g_a(x) &= g(x) - a\]
% so we're dropping the non-negativity requirement too? what does majorization mean if they aren't nonnegative? and how does that work with jensen.
% need to be clearer about this so we can get jensen out of it

% i think we can get rid of continuity requirement.  for left end approximating with psi_n already works
% what about for right end?

% can we also generalize to locally-convex topological vector space instead of real-valued?  where does the nonnegativity come in?
% would also need hahn-banach in that case...

From this we can derive familiar results:

\begin{theorem}[Jensen's Inequality]
  Let $X$ be a random variable and $\phi$ a continuous convex function. % xxx shouldnt need phi continuous, also be careful about nonnegativity in majorization.  also might be easier to derive in the other direction
  Then $\phi(\E X) \le \E[\phi(X)]$.
\end{theorem}
\begin{proof}
  Consider the constant random variable $Y(x) = \E X$.
  Then $Y \prec X$ and so by Theorem~\ref{thm:majorize} we have
  $\phi(\E X) = \E[\phi(Y)] \le \E[\phi(X)]$.
\end{proof}

For random variables, $Y \prec X$ intuitively means that $Y$ is more concentrated than $X$; for instance we have $\Var Y \le \Var X$ since $x \mapsto x^2$ is convex.  Similarly the best Chernoff bound for $X$ is also a bound on $Y$, since $x \mapsto \exp(\lambda x)$ is convex.

% todo: material from Statistical Orders, especially Markov kernel formulation

The following observation can slightly simplify uses of convex ordering:

\begin{lemma}
  \label{lem:rhs-suffices}
  Let $X, Y$ be random variables such that $\E[f(X)] \le \E[f(Y)]$ for any convex function $f$
  whenever both expectations exist.
  Then in fact $\E[f(X)]$ exists whenever $\E[f(Y)]$ does.
\end{lemma}

To prove this we first show:

\begin{lemma}\label{lem:asymptote}
  Suppose $f$ is an increasing function with an oblique asymptote as $x \to \infty$;
  that is, there is a function $g(x) = cx + d$ such that $\lim_{x \to \infty} |f(x) - g(x)| = 0$, where $c > 0$.
  Then for any random variable $W$ we have $\E[f(W)^+] = \infty$ if and only if $\E[W^+] = \infty$.
\end{lemma}
\begin{proof}
  Without loss of generality we can assume $f(0) = 0$.
  Let $M = \sup_{x \ge 0} |f(x) - cx| < \infty$.
  Then \[\E[|f(W)^+ - cW^+|] \le \E[M] = M\]
  implies the statement via the triangle inequality.
\end{proof}

\begin{proof}[Proof of \cref{lem:rhs-suffices}]
  Fix a convex function $f$.
  It suffices to show the implications
  \[
  \E[f(Y)^+] < \infty &\to \E[f(X)^+] < \infty \\
  \E[f(Y)^-] < \infty &\to \E[f(X)^-] < \infty \\
  \]

  The first implication is easy to prove since we have $\E[f(X)^+] \le \E[f(Y)^+]$
  by convexity of $f^+$.

  Now, if $f$ is bounded from below then $\E[f(X)^-] < \infty$ trivially, so we can assume this isn't the case.
  Since $f$ is convex, this means it has a decreasing oblique asymptote as $x \to -\infty$ or as $x \to \infty$; without loss of generality assume it's the former.

  From $\E[f(Y)^-] < \infty$, we know by \cref{lem:asymptote} that $\E[f(Y)^-] < \infty$ also.
  But then $\E[X^-] \le \E[{Y}^-]$ by convexity of $x \mapsto x^-$,
  and so finally $\E[f(X)^-]$ by \cref{lem:asymptote} again.
\end{proof}

\section*{Negative Association}

% see: NA notes

We prove Shao's comparison theorem for negatively associated random variables.  The first part of the proof is identical to Shao's, but we give a nicer proof for the second part.  We also impose some additional conditions needed to apply Fubini's theorem in the proof.

% cite: A Comparison Theorem on Moment Inequalities Between Negatively Associated and Independent Random Variables 🔍
% Springer US; Springer-Verlag; Kluwer Academic Publishers; Springer Science and Business Media LLC (ISSN 0894-9840), Journal of Theoretical Probability, #2, 13, pages 343-356, 2000
% Qi-Man Shao

\begin{definition}[Negative Association]
  A set $\{X_i : i \in T\}$ of random variables is \defn{negatively associated} if for any pair of finite disjoint subsets $A, B \subset T$
  we have
  \[\E[f_A(X_i: i \in A)f_B(X_i: i \in B)] \le \E[f_A(X_i: i \in A)]\E[f_B(X_i: i \in B)]\]
  where $f_A : \R^{|A|} \to \R$ and $f_B : \R^{|B|} \to \R$ are increasing functions.
\end{definition}

\begin{fact}[Monotonicity Property]
  Let $\{X_i : i \in T\}$ be a set of negatively associated random variables,
  and let $P$ be a partition of $T$.
  Then $\{f_p(X_i : i \in p) : p \in P\}$ are negatively associated,
  where each $f_p$ is an increasing function $\R^{|p|} \to \R$.
\end{fact}

\begin{theorem}
  \label{thm:na-convex}
  Suppose $X_1, \dots, X_n$ are negatively associated.
  Let $X_1^*, \dots, X_n^*$ be independent random variables with the same distribution as $X_1, \dots, X_n$.
  \begin{thmenum}
  \item \label{thm:na-convex-sum} If $f : \R \to \R$ is convex, then
    \[\E\left[f\left(\sum_i X_i\right)\right] \le \E\left[f\left(\sum_i X_i^*\right)\right]\]
    provided that the expectation on the right-hand side exists.

  \item \label{thm:na-convex-maxsum}
    If $f : \R \to \R$ is convex and increasing, then
    \[\E\left[f\left(\max_{1 \le k \le n} \sum_{i=1}^k X_i\right)\right] \le \E\left[f\left(\max_{1 \le k \le n} \sum_{i=1}^k X_i^*\right)\right]\]
    provided that the expectation on the right-hand side exists.
  \end{thmenum}
\end{theorem}

We build up the proof of this theorem in stages.  We start with just the first part for $n = 2$ and assume the random variables involved are bounded.

\begin{proof}[Proof of \cref{thm:na-convex-sum} for two bounded variables.]
  Let $Y_1, Y_2$ have the same joint distribution as $X_1, X_2$ with $\{X_1, X_2\} \perp \{Y_1, Y_2\}$.
  We can write
  \[
  f(X_1 + X_2) + f(Y_1 + Y_2) - f(X_1 + Y_2) - f(Y_1 + X_2)
  &= \int_{X_2}^{Y_2} (f'(Y_1 + t) - f'(X_1 + t))\,dt \\
  &= \int_{-\infty}^\infty (f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])\,dt \\
  \]
  and so
  \[
  \E[f(X_1 + X_2)] - \E[f(X_1^* + X_2^*)]
  &= \frac12\E[f(X_1 + X_2) + f(Y_1 + Y_2) - f(X_1 + Y_2) - f(Y_1 + X_2)] \\
  &= \frac12\E\left[\int_{-\infty}^\infty (f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])\,dt \right] \\
  &= \frac12\int_{-\infty}^\infty \E[(f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])]\,dt &\text{by Fubini} \\
  &= \int_{-\infty}^\infty \Cov(f'(X_1 + t), [t < X_2])\,dt \\
  &\le 0
  \]
  where the last inequality is because $f'(X_1 + t)$ and $[t < X_2]$ are negatively associated for each $t$ by monotonicity.
  The use of Fubini's theorem is justified since the integrand is absolutely integrable:
  \[
  &\pheq \frac12\E\left[\int_{-\infty}^\infty |(f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])|\,dt \right] \\
  &= \frac12\E\left[\left|\int_{-\infty}^\infty (f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])\,dt \right|\right] \\
  &= \frac12\E[|f(X_1 + X_2) + f(Y_1 + Y_2) - f(X_1 + Y_2) - f(Y_1 + X_2)|] \\
  &\le \E[|f(X_1 + X_2)|] + \E[|f(X_1^* + X_2^*)|] < \infty & \text{$X_i$ bounded}
  \]
  where the first equality is because both $x \mapsto f'(x + t)$ and $x \mapsto [t < x]$ are increasing,
  so for fixed $X_1, Y_1, X_2, Y_2$, the integrand has the same sign for all $t$.
\end{proof}

Now we extend this to arbitrary random variables.

\begin{lemma}\label{lem:monotone-convergence-2}
  Let $A$ be a measurable set in a measure space $(S, \Lambda, \mu)$,
  and let $f_1, f_2, \dots$ be a sequence of measurable functions
  such that $f_i$ is increasing in $i$ on $A$ and decreasing on $i$ outside of $A$.
  Assume that $\int |f_1| d\mu < \infty$, the $f_i$ converge to a pointwise limit,
  and $\int \lim_i f_i\,d\mu$ exists.
  Then
  \[
  \int \lim_i f_i\,d\mu = \lim_i \int f_i\,d\mu
  \]
\end{lemma}
\begin{proof}
    \[
  \int \lim_i f_i\,d\mu
  &= \int_A \lim_i f_i\,d\mu + \int_{\overline A} \lim_i f_i\,d\mu \\
  &= \lim_i \int_A f_i\,d\mu + \lim_i \int_{\overline A} f_i\,d\mu &\text{by MCT} \\
  &= \lim_i \int_A f_i\,d\mu \\
  \]
\end{proof}

\begin{proof}[Proof of \cref{thm:na-convex-sum} for two random variables.]
  By \cref{lem:rhs-suffices} we can assume that both $\E[f(X_1 + X_2)]$ and $\E[f(X_1^* + X_2^*)]$ exist.
  For $i \in \N$ define
  \[
  S_i &= ((X_1 \vee -i) \wedge i) + ((X_2 \vee -i) \wedge i) \\
  S_i^* &= ((X_1^* \vee -i) \wedge i) + ((X_2^* \vee -i) \wedge i)
  \]
  By the previous proof we have
  \[
  \E[f(X_1 + X_2)] = \E[\lim_i f(S_i)] = \lim_i \E[f(S_i)] \le \lim_i \E[f(S_i^*)] = \E[\lim_i f(S_i)] = \E[f(X_1^* + X_2^*)]
  \]
  provided that we can show that the limits and expectations commute.

  Since $f$ is convex, it is either bounded from below or monotone.
  If $f$ is bounded from below then 
  $f(S_i)$ and $f(S_i^*)$ are eventually increasing in $i$, so the limits and expectations commute by the monotone convergence theorem.

  Otherwise, $f$ is monotone; without loss of generality assume it is increasing.
  Define the event $A = [X_1 \ge X_2]$.
  Then $S_i$ (and thus $f(S_i)$) is increasing in $i$ on $A$ and
  decreasing in $i$ on $\overline A$.
  So $\E[\lim_i f(S_i)] = \lim_i \E[f(S_i)]$ by \cref{lem:monotone-convergence-2}, and the same reasoning works for $S_i^*$.
\end{proof}

Now we can prove the rest of the theorem.

\begin{proof}[Proof of \cref{thm:na-convex-sum}.]
  For $n > 2$ we induct on $n$.
  Let $X_1, \dots, X_{n+1}$ be negatively associated.
  Define $S = \sum_{i=2}^{n+1} X_i$
  and $S^* = \sum_{i=2}^{n+1} X_i^*$.
  We have
  \[
  \E\left[f\left(\sum_{i=1}^{n+1} X_i\right)\right]
  &= \E[f(X_1 + S)] \\
  &\le \E[f(X_1^* + S)] &\text{since $X_1, S$ are NA} \\
  &= \E[(c \mapsto \E[f(c + S)])(X_1^*)] \\
  &= \E[(c \mapsto \E[f(c + S^*)])(X_1^*)] &\text{by inductive hypothesis} \\
  &= \E[f(X_1^* + S^*)]] \\
  &= \E\left[f\left(\sum_{i=1}^{n+1} X_i^*\right)\right] \\
  \]
  Here the inductive hypothesis is applied using the function $x \mapsto f(c + x)$ which is convex for every value of $c$.
\end{proof}

The proof for the second part is very similar.

\begin{proof}[Proof of \cref{thm:na-convex-maxsum}.]
  The statement for $n = 2$ follows easily from the first part
  since $X_1$ and $X_2^+$ are negatively associated:
  \[
  \E[f(\max\{X_1, X_1 + X_2\})]
  &= \E[f(X_1 + X_2^+)] \\
  &\le \E[f(X_1^* + {X_2^*}^+)] \\
  &= \E[f(\max\{X_1^*, X_1^* + X_2^*\})] \\
  \]

  For $n > 2$ we induct on $n$.
  Let $X_1, \dots, X_{n+1}$ be negatively associated.
  Define $L = \max_{2 \le k \le n+1} \sum_{i=2}^k X_i$
  and $L^* = \max_{2 \le k \le n+1} \sum_{i=2}^k X_i^*$.
  Then we have:
  \[
  \E\left[f\left(\max_{1 \le i \le n+1} \sum_i X_i\right)\right]
  &= \E[f(X_1 + L^+)] \\
  &\le \E[f(X_1^* + L^+)] &\text{since $X_1, L^+$ are NA} \\
  &= \E[(c \mapsto \E[f(c + L^+)])(X_1^*)]] \\
  &\le \E[(c \mapsto \E[f(X_1^* + {L^*}^+))(X_1^*)] & \text{by inductive hypothesis} \\
  &= \E[f(X_1^* + {L^*}^+)] \\
  &= \E\left[f\left(\max_{1 \le i \le n+1} \sum_i X_i^*\right)\right]
  \]
  Here the inductive hypothesis is applied using the function $x \mapsto f(c + x^+)$ which is convex because $f$ is increasing and convex.
\end{proof}

\end{document}

