\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[colorlinks, linkcolor=purple, citecolor=blue!75!black, urlcolor=purple]{hyperref}

\usepackage{libertine}
\usepackage[libertine,vvarbb]{newtxmath}
\usepackage[scaled=0.96]{zi4}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{claim}[theorem]{Claim}

\newcommand*{\defn}[1]{\textbf{#1}}
\newcommand*{\N}[0]{\mathbb{N}}
\newcommand*{\R}[0]{\mathbb{R}}
\newcommand*{\E}[0]{\mathbf{E}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand*{\pheq}{\mathrel{\phantom{=}}} % invisible = for align mode

\def\[#1\]{\begin{align*}#1\end{align*}}

\title{Notes on Convexity Inequalities}
\author{Lily Chung}
\date{}

\begin{document}
\maketitle

These are some notes on inequalities arising from convexity---currently unfinished/wrong/disorganized.

In the following $(S, \Lambda, \mu)$ will be a $\sigma$-finite measure space.
We will mostly be interested in the cases where $S$ is a probability space ($\mu(S) = 1$) or where $S = \N$ and $\mu$ is counting measure.

\begin{definition}
  Let $T$ be a convex subset of $\R$.
  A real-valued function $f : T \to \R$ is \defn{convex} if for any $a, b \in T$ and any $t \in [0, 1]$ we have $f(ta + (1 - t)b) \le tf(a) + (1 - t)f(b)$.
\end{definition}

It can be checked that if $f$ is convex on $[a, b]$ then it is continuous on $(a, b)$, left-differentiable on $(a, b]$, and right-differentiable on $[a, b)$, the derivatives are increasing, and $f(y) - f(x) = \int_x^y f'(t)\,dt$ where $x, y \in (a, b)$ and $f'$ is either derivative of $f$.

%% Let $h: S \to \R_{\ge 0}$ be a nonnegative measurable function.
%% We define functions $m_h : \R_{\ge 0} \to [0, \mu(S)]$ and $h^* : [0, \mu(S)] \to \R_{\ge 0}$ via
%% \[
%% m_h(t) &= \mu(\{x : h(x) > t\}) \\
%% h^*(u) &= \sup\{t : m_h(t) > u\}
%% \]

%% We can think of $h^*$ as a ``decreasing rearrangement'' of $h$; it is a decreasing measurable function on $[0, \mu(S)]$ with the same distribution as $h$.

\begin{definition}
  Let $f, g : S \to \R_{\ge 0}$ be nonnegative measurable functions such that $\int f\,d\mu = \int g\,d\mu < \infty$.
  We say $g$ \defn{majorizes} $f$ if any of the following equivalent conditions hold:
  \begin{itemize}
  \item $\int \max\{f - t, 0\}\,d\mu \le \int \max\{g - t, 0\}\,d\mu$ for every $t \ge 0$.
  \item $\int_t^\infty m_f(u)\,du \le \int_t^\infty m_g(u)\,du$ for all $t \ge 0$, where $m_h(u) = \mu(\{x : h(x) > u\})$.
  \item $\int_0^r f^*(\ell)\,d\ell \le \int_0^r g^*(\ell)\,d\ell$ for all $0 \le r < \mu(S)$, where $h^*(\ell) = \sup\{t : m_h(t) > \ell\}$.
  \end{itemize}
\end{definition}

We write $f \prec g$ to mean $g$ majorizes $f$.
We prove a technical lemma:

\begin{lemma}\label{lem:attain max}
  Suppose $f \prec g$, and let $b = \max\{\esssup f, \esssup g\}$.  Then $\mu(\{x : f(x) = b\}) \le \mu(\{x : g(x) = b\})$.
\end{lemma}
\begin{proof}
  If $\esssup f \ne \esssup g$ the result is trivial.
  For $h \in \{f, g\}$ and $\epsilon > 0$ we have
  \[
  \int \max\{h - (b - \epsilon), 0\}\,d\mu &\in \epsilon[\mu(h = b), \mu(h \ge b - \epsilon)] \\
  \frac{\int \max\{h - (b - \epsilon), 0\}\,d\mu}{\epsilon} &\in \mu(h = b) + [0, \mu(b - \epsilon \le h < b)] \\
  \lim_{\epsilon \to 0^+} \frac{\int \max\{h - (b - \epsilon), 0\}\,d\mu}{\epsilon} &= \mu(h = b)
  \]
  But $\frac{\int \max\{f - (b - \epsilon), 0\}\,d\mu}{\epsilon} \le \frac{\int \max\{g - (b - \epsilon), 0\}\,d\mu}{\epsilon}$ for all $\epsilon > 0$
  and so $\mu(f = b) \le \mu(g = b)$.
\end{proof}


\begin{theorem}[Theorem 2.1 in Joe 1987]
  \label{thm:majorize}
  Let $\phi$ be a convex function on an interval $T \subset \R$ containing 0
  such that $\phi(0) = 0$.
  Let $f \prec g$ where $f, g$ take values in $T$.
  Then \[\int (\phi \circ f)\,d\mu \le \int (\phi \circ g)\,d\mu\]
  provided that the integrals exist and are finite.
\end{theorem}
\begin{proof}
  First we consider the case where $\phi$ is continuous and $\phi'(0^+) > -\infty$.  In this case, we can approximate $\phi$ by an increasing sequence of finite sums of the form $\phi_n(u) = \phi'(0^+)u + \sum_i a_i \max\{u - b_i, 0\}$ where $a_i, b_i \ge 0$.  Linearity gives us $\int (\phi_n \circ f)\,d\mu \le \int (\phi_n \circ g)\,d\mu$ and the result follows from the monotone convergence theorem.  In this case we don't need the condition that the integrals are finite: they always exist and the result holds even if one or both is $\infty$.

  For the next case we allow $\phi'(0^+) = -\infty$, and assume $\phi$ is continuous at its right endpoint but not necessarily its left.
  We approximate $\phi$ by a sequence of continuous convex functions
  \[\psi_n(u) = 
  \begin{cases}
    n\phi(n^{-1})u & 0 \le u < n^{-1}\\
    \phi(u) & u \ge n^{-1}
  \end{cases}\]
  where each $\psi_n'(0^+) > -\infty$.
  By the first case we have $\int (\psi_n \circ f)\,d\mu \le \int (\psi_n \circ g)\,d\mu$ and we can finish using the dominated convergence theorem since $|\psi_n| \le |\phi|$.

  Finally we handle the case where $\phi$ is discontinuous at its right endpoint, which we call $b$.  We can write $\phi(u) = \widetilde\phi(u) + c[u = b]$ for some $c > 0$ and some $\widetilde\phi$ which satisfies the conditions of the previous case.
  The result then follows from Lemma~\ref{lem:attain max}.
\end{proof}

Note that if $\mu(S) < \infty$ we can work with a general interval $[a, b]$ and dispense with the assumption $\phi(0) = 0$,
by considering the functions \[\phi_a(u) &= \phi(u + a) - \phi(a) \\ f_a(x) &= f(x) - a \\ g_a(x) &= g(x) - a\]
% so we're dropping the non-negativity requirement too? what does majorization mean if they aren't nonnegative? and how does that work with jensen.
% need to be clearer about this so we can get jensen out of it

% i think we can get rid of continuity requirement.  for left end approximating with psi_n already works
% what about for right end?

% can we also generalize to locally-convex topological vector space instead of real-valued?  where does the nonnegativity come in?
% would also need hahn-banach in that case...

From this we can derive familiar results:

\begin{theorem}[Jensen's Inequality]
  Let $X$ be a random variable and $\phi$ a continuous convex function. % xxx shouldnt need phi continuous, also be careful about nonnegativity in majorization.  also might be easier to derive in the other direction
  Then $\phi(\E X) \le \E[\phi(X)]$.
\end{theorem}
\begin{proof}
  Consider the constant random variable $Y(x) = \E X$.
  Then $Y \prec X$ and so by Theorem~\ref{thm:majorize} we have
  $\phi(\E X) = \E[\phi(Y)] \le \E[\phi(X)]$.
\end{proof}

For random variables, $Y \prec X$ intuitively means that $Y$ is more concentrated than $X$; for instance we have $\Var Y \le \Var X$ since $x \mapsto x^2$ is convex.  Similarly the best Chernoff bound for $X$ is also a bound on $Y$, since $x \mapsto \exp(\lambda x)$ is convex.

% todo: material from Statistical Orders, especially Markov kernel formulation

\section*{Negative Association}

% see: NA notes

We prove Shao's comparison theorem for negatively associated random variables.  The first part of the proof is identical to Shao's, but we give a nicer proof for the second part.  We also impose some additional conditions needed to apply Fubini's theorem in the proof.

% cite: A Comparison Theorem on Moment Inequalities Between Negatively Associated and Independent Random Variables ðŸ”
% Springer US; Springer-Verlag; Kluwer Academic Publishers; Springer Science and Business Media LLC (ISSN 0894-9840), Journal of Theoretical Probability, #2, 13, pages 343-356, 2000
% Qi-Man Shao

\begin{definition}[Negative Association]
  A set $\{X_i : i \in T\}$ of random variables is \defn{negatively associated} if for any pair of finite disjoint subsets $A, B \subset T$
  we have
  \[\E[f_A(X_i: i \in A)f_B(X_i: i \in B)] \le \E[f_A(X_i: i \in A)]\E[f_B(X_i: i \in B)]\]
  where $f_A : \R^{|A|} \to \R$ and $f_B : \R^{|B|} \to \R$ are increasing functions.
\end{definition}

\begin{fact}[Monotonicity Property]1
  Let $\{X_i : i \in T\}$ be a set of negatively associated random variables,
  and let $P$ be a partition of $T$.
  Then $\{f_p(X_i : i \in p) : p \in P\}$ are negatively associated,
  where each $f_p$ is an increasing function $\R^{|p|} \to \R$.
\end{fact}

\begin{theorem}
  Suppose $X_1, \dots, X_n$ are negatively associated.
  Let $X_1^*, \dots, X_n^*$ be independent random variables with the same distribution as $X_1, \dots, X_n$.
  Then if $f$ is a convex function,
  \[\E\left[f\left(\sum_i X_i\right)\right] \le \E\left[f\left(\sum_i X_i^*\right)\right]\]
  provided that for each $0 \le r \le n$, the expectation $\E\left[f\left(\sum_{i=1}^k Z_i\right)\right]$ is finite, where $Z_i = 
  \begin{cases}
    X_i^* & i \le r \\
    X_i & i > r
  \end{cases}$.
  Furthermore if either $n = 2$ or $f$ is increasing,
  \[\E\left[f\left(\max_{1 \le k \le n} \sum_{i=1}^k X_i\right)\right] \le \E\left[f\left(\max_{1 \le k \le n} \sum_{i=1}^k X_i^*\right)\right]\]
  provided that each $\E\left[f\left(\max_{1 \le k \le n} \sum_{i=1}^k Z_i\right)\right]$ is finite.
\end{theorem}
\begin{proof}[Proof of first part.]
  We start with the case where $n = 2$.
  Let $Y_1, Y_2$ have the same joint distribution as $X_1, X_2$ with $\{X_1, X_2\} \perp \{Y_1, Y_2\}$.
  We can write
  \[
  f(X_1 + X_2) + f(Y_1 + Y_2) - f(X_1 + Y_2) - f(Y_1 + X_2)
  &= \int_{X_2}^{Y_2} (f'(Y_1 + t) - f'(X_1 + t))\,dt \\
  &= \int_{-\infty}^\infty (f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])\,dt \\
  \]
  and so
  \[
  \E[f(X_1 + X_2)] - \E[f(X_1^* + X_2^*)]
  &= \frac12\E[f(X_1 + X_2) + f(Y_1 + Y_2) - f(X_1 + Y_2) - f(Y_1 + X_2)] \\
  &= \frac12\E\left[\int_{-\infty}^\infty (f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])\,dt \right] \\
  &= \frac12\int_{-\infty}^\infty \E[(f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])]\,dt &\text{by Fubini} \\
  &= \int_{-\infty}^\infty \Cov(f'(X_1 + t), [t < X_2])\,dt \\
  &\le 0
  \]
  where the last inequality is because $f'(X_1 + t)$ and $[t < X_2]$ are negatively associated for each $t$ by monotonicity.
  The use of Fubini's theorem is justified since the integrand is absolutely integrable:
  \[
  &\pheq \frac12\E\left[\int_{-\infty}^\infty |(f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])|\,dt \right] \\
  &= \frac12\E\left[\left|\int_{-\infty}^\infty (f'(Y_1 + t) - f'(X_1 + t))([t < Y_2] - [t < X_2])\,dt \right|\right] \\
  &= \frac12\E[|f(X_1 + X_2) + f(Y_1 + Y_2) - f(X_1 + Y_2) - f(Y_1 + X_2)] \\
  &\le \E[|f(X_1 + X_2)|] + \E[|f(X_1^* + X_2^*)|] < \infty & \text{by assumption}
  \]
  where the first equality is because both $x \mapsto f'(x + t)$ and $x \mapsto [t < x]$ are increasing,
  so for fixed $X_1, Y_1, X_2, Y_2$, the integrand has the same sign for all $t$.

  For $n > 2$ we induct on $n$.
  Let $X_1, \dots, X_{n+1}$ be negatively associated.
  Define $S = \sum_{i=2}^{n+1} X_i$
  and $S^* = \sum_{i=2}^{n+1} X_i^*$.
  We have
  \[
  \E\left[f\left(\sum_{i=1}^{n+1} X_i\right)\right]
  &= \E[f(X_1 + S)] \\
  &\le \E[f(X_1^* + S)] &\text{since $X_1, S$ are NA} \\
  &= \E[\E[f(X_1^* + S) \mid X_1^*]] \\
  &\le \E[\E[f(X_1^* + S^*) \mid X_1^*]] &\text{by inductive hypothesis} \\
  &= \E[f(X_1^* + S^*)]] \\
  &= \E\left[f\left(\sum_{i=1}^{n+1} X_i^*\right)\right] \\
  \]
  Here the inductive hypothesis is applied using the function $x \mapsto f(X_1^* + x)$ which is convex for every value of $X_1^*$.
\end{proof}

The proof for the second part is very similar.

\begin{proof}[Proof of second part.]
  The statement for $n = 2$ follows easily from the first part
  since $X_1$ and $X_2^+$ are negatively associated:
  \[
  \E[f(\max\{X_1, X_1 + X_2\})]
  &= \E[f(X_1 + X_2^+)] \\
  &\le \E[f(X_1^* + {X_2^*}^+)] \\
  &= \E[f(\max\{X_1^*, X_1^* + X_2^*\})] \\
  \]

  For $n > 2$ we induct on $n$.
  Let $X_1, \dots, X_{n+1}$ be negatively associated.
  Define $L = \max_{2 \le k \le n+1} \sum_{i=2}^k X_i$
  and $L^* = \max_{2 \le k \le n+1} \sum_{i=2}^k X_i^*$.
  Then we have:
  \[
  \E\left[f\left(\max_{1 \le i \le n+1} \sum_i X_i\right)\right]
  &= \E[f(X_1 + L^+)] \\
  &\le \E[f(X_1^* + L^+)] &\text{since $X_1, L^+$ are NA} \\
  &= \E[\E[f(X_1^* + L^+) \mid X_1^*]] \\
  &\le \E[\E[f(X_1^* + {L^*}^+) \mid X_1^*]] & \text{by inductive hypothesis} \\
  &= \E[f(X_1^* + {L^*}^+)] \\
  &= \E\left[f\left(\max_{1 \le i \le n+1} \sum_i X_i^*\right)\right]
  \]
  Here the inductive hypothesis is applied using the function $x \mapsto f(X_1^* + x^+)$ which is convex because $f$ is increasing and convex.
  % what about fubini conditions??
\end{proof}

\end{document}

