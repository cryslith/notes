\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}

\usepackage{libertine}
\usepackage[libertine,vvarbb]{newtxmath}
\usepackage[scaled=0.96]{zi4}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{claim}[theorem]{Claim}

\newcommand*{\defn}[1]{\textbf{#1}}
\newcommand*{\N}[0]{\mathbb{N}}
\newcommand*{\R}[0]{\mathbb{R}}
\newcommand*{\E}[0]{\mathbf{E}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}


\def\[#1\]{\begin{align*}#1\end{align*}}

\title{Notes on Majorization Inequalities}
\author{Lily Chung}
\date{}

\begin{document}
\maketitle

These are some notes on inequalities arising from majorization.
We start with a very general majorization inequality and use this to obtain many useful and well-known results.
I collected these so that I could remember how to derive these results without circularity.

In the following $(S, \Lambda, \mu)$ will be a measure space.
We will mostly be interested in the cases where $S$ is a probability space ($\mu(S) = 1$) or where $S = \N$ and $\mu$ is counting measure.

\begin{definition}
  A real-valued function $f$ is \defn{convex} on an interval $[a, b]$ if for all $t \in [0, 1]$ we have $f(ta + (1 - t)b) \le tf(a) + (1 - t)f(b)$.
\end{definition}

It can be checked that if $f$ is convex on $[a, b]$ then it is continuous on $(a, b)$, left-differentiable on $(a, b]$, and right-differentiable on $[a, b)$.

%% \section{Majorization on measure spaces}

%% The following summary of majorization on measure spaces is from (Joe, 1987).
%% Let $h: S \to \R_{\ge 0}$ be a nonnegative measurable function.
%% We define functions $m_h : \R_{\ge 0} \to [0, \mu(S)]$ and $h^* : [0, \mu(S)] \to \R_{\ge 0}$ via
%% \[
%% m_h(t) &= \mu(\{x : h(x) > t\}) \\
%% h^*(u) &= \sup\{t : m_h(t) > u\}
%% \]

%% We can think of $h^*$ as a ``decreasing rearrangement'' of $h$; it has the same mass distribution as $h$ but is a decreasing function on $\R$.
%% Now for the main theorem:

\begin{definition}
  Let $f, g : S \to \R_{\ge 0}$ be nonnegative integrable functions such that $\int f\,d\mu = \int g\,d\mu < \infty$.
  We say $g$ \defn{majorizes} $f$ if any of the following equivalent conditions hold:
  \begin{itemize}
  \item $\int \max\{f - t, 0\}\,d\mu \le \int \max\{g - t, 0\}\,d\mu$ for every $t \ge 0$.
  \item $\int_t^\infty m_f(u)\,du \le \int_t^\infty m_g(u)\,du$ for all $t \ge 0$, where $m_h(u) = \mu(\{x : h(x) > u\})$.
  \item $\int_0^r f^*(\ell)\,d\ell \le \int_0^r g^*(\ell)\,d\ell$ for all $0 \le r < \mu(S)$, where $h^*(\ell) = \sup\{t : m_h(t) > \ell\}$.
  \end{itemize}
  We write $f \prec g$ to mean $g$ majorizes $f$.
\end{definition}

\begin{theorem}[Theorem 2.1 in Joe 1987]
  \label{thm:majorize}
  Let $\phi$ be a continuous convex function on an interval $[0, b]$
  such that $\phi(0) = 0$.
  Let $f \prec g$ where $f, g$ take values in $[0, b]$.
  Then \[\int (\phi \circ f)\,d\mu \le \int (\phi \circ g)\,d\mu\]
  provided that the integrals exist and are finite.
\end{theorem}
\begin{proof}
  First we consider the case where $\phi'(0^+) > -\infty$.  In this case, we can approximate $\phi$ by an increasing sequence of finite sums of the form $\phi_n(u) = \sum_i a_i \max\{u - b_i, 0\}$.  Linearity gives us $\int (\phi_n \circ f)\,d\mu \le \int (\phi_n \circ g)\,d\mu$ and the result follows from the monotone convergence theorem.  In this case we don't need the condition that the integrals are finite: they always exist and the result holds even if one or both is $\infty$.

  In the case where $\phi'(0^+) = -\infty$ we approximate $\phi$ by a sequence of convex functions
  \[\psi_n(u) = 
  \begin{cases}
    n\phi(n^{-1})u & 0 \le u < n^{-1}\\
    \phi(u) & u \ge n^{-1}
  \end{cases}\]
  where each $\psi_n'(0^+) > -\infty$.
  By the first case we have $\int (\psi_n \circ f)\,d\mu \le \int (\psi_n \circ g)\,d\mu$ and we can finish using the dominated convergence theorem since $|\psi_n| \le |\phi|$.
\end{proof}

Note that if $\mu(S) < \infty$ we can work with a general interval $[a, b]$ and dispense with the assumption $\phi(0) = 0$,
by considering the functions \[\phi_a(u) &= \phi(u + a) - \phi(a) \\ f_a(x) &= f(x) - a \\ g_a(x) &= g(x) - a\]

From this we can derive familiar results:

\begin{theorem}[Jensen's Inequality]
  Let $X$ be a random variable and $\phi$ a continuous convex function.
  Then $\phi(\E X) \le \E[\phi(X)]$.
\end{theorem}
\begin{proof}
  Consider the constant random variable $Y(x) = \E X$.
  Then $Y \prec X$ and so by Theorem~\ref{thm:majorize} we have
  $\phi(\E X) = \E[\phi(Y)] \le \E[\phi(X)]$.
\end{proof}

For random variables, $Y \prec X$ intuitively means that $Y$ is more concentrated than $X$; for instance we have $\Var Y \le \Var X$ since $x \mapsto x^2$ is convex.

\section*{Power Means}

Let $X$ be a nonnegative random variable, and define:

\[M_p(X) =
\begin{cases}
  \E[X^p]^{1/p} & p \in \R - \{0\} \\
  \exp \E[\log X] & p = 0 \\
  \essinf(X) & p = -\infty \\
  \esssup(X) & p = \infty \\
\end{cases}\]

\begin{theorem}[Power Mean Inequality]
  If $p \le q$ then $M_p(X) \le M_q(X)$ whenever both exist.
\end{theorem}
\begin{proof}
  If either of $p$ or $q$ is infinite then the result is trivial.

  Suppose $0 = p < q$.  Since $\log$ is concave we have by Jensen's inequality
  \[
  \log \E[X^p] &\ge \E[\log X^p] \\
  \frac1p \log \E[X^p] &\ge \E[\log X] \\
  \E[X^p]^{1/p} &\ge \exp \E[\log X] \\
  \]
  The case where $p < q = 0$ is analogous.

  Now suppose $0 < p < q$.  Since $u \mapsto u^{q/p}$ is convex we have
  \[
  \E[X^p]^{q/p} &\le \E[X^q] \\
  \E[X^p]^{1/p} &\le \E[X^q]^{1/q} \\
  \]
  The case where $p < q < 0$ is analogous.
\end{proof}

% xxx need to prove existence of M_0 so there's no issue with transitivity

For instance this generalizes the AM-GM inequality since $M_0$ is the geometric mean and $M_1$ is the arithmetic mean.

\section*{H\"older's Inequality}

Let $f : S \to \R$ be a nonnegative measurable function.
Define \[\norm{f}_p = \begin{cases}
  \left(\int f\,d\mu\right)^{1/p} & 1 \le p < \infty \\
  \esssup f & p = \infty \\
  \end{cases}
\]
Note that the results from the previous section do not directly apply here since $S$ is not necessarily a probability space.  Nonetheless we can make use of them by defining an appropriate probability measure.

\begin{theorem}[H\"older's Inequality]
  Let $f, g : S \to \R$ be nonnegative measurable functions,
  and let $p, q \in [1, \infty]$ such that $\frac1p + \frac1q = 1$.
  Then $\norm{fg}_1 \le \norm{f}_p \norm{g}_q$.
\end{theorem}
\begin{proof}
  The case where $p$ or $q$ is $\infty$ is trivial, so we skip it.
  Define $\nu$ to be a probability measure on $S$ via $\nu(A) = \frac{\int_A g^q\,d\mu}{\int g^q\,d\mu}$,
  and let $X = fg^{1-q}$.
  Then we have
  \[\norm{fg}_1 &= \left(\int g^q\,d\mu\right)\left(\int f g^{1-q} \frac{g^q}{\int g^q\,d\mu}\,d\mu\right) \\
  &= \left(\int g^q\,d\mu\right)\E_\nu[X] \\
  &\le \left(\int g^q\,d\mu\right)\E_\nu[X^p]^{1/p} \\
  &= \left(\int g^q\,d\mu\right)\left(\int f^p g^{p(1-q)}\frac{g^q}{\int g^q\,d\mu}\,d\mu \right)^{1/p} \\
  &= \left(\int g^q\,d\mu\right)^{1 - 1/p}\left(\int f^p g^{p + q - pq}\,d\mu \right)^{1/p} \\
  &= \norm{f}_p \norm{g}_q \\
\]
\end{proof}

\end{document}
