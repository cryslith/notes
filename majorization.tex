\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[colorlinks, linkcolor=purple, citecolor=blue!75!black, urlcolor=purple]{hyperref}

\usepackage{libertine}
\usepackage[libertine,vvarbb]{newtxmath}
\usepackage[scaled=0.96]{zi4}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{claim}[theorem]{Claim}

\newcommand*{\defn}[1]{\textbf{#1}}
\newcommand*{\N}[0]{\mathbb{N}}
\newcommand*{\R}[0]{\mathbb{R}}
\newcommand*{\E}[0]{\mathbf{E}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\essinf}{ess\,inf}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}


\def\[#1\]{\begin{align*}#1\end{align*}}

\title{Notes on Convexity Inequalities}
\author{Lily Chung}
\date{}

\begin{document}
\maketitle

These are some notes on inequalities arising from convexity.
We start with a general inequality and use this to obtain more familiar results.

In the following $(S, \Lambda, \mu)$ will be a $\sigma$-finite measure space.
We will mostly be interested in the cases where $S$ is a probability space ($\mu(S) = 1$) or where $S = \N$ and $\mu$ is counting measure.

\begin{definition}
  Let $T$ be a convex subset of $\R$.
  A real-valued function $f : T \to \R$ is \defn{convex} if for any $a, b \in T$ and any $t \in [0, 1]$ we have $f(ta + (1 - t)b) \le tf(a) + (1 - t)f(b)$.
\end{definition}

It can be checked that if $f$ is convex on $[a, b]$ then it is continuous on $(a, b)$, left-differentiable on $(a, b]$, and right-differentiable on $[a, b)$.

%% Let $h: S \to \R_{\ge 0}$ be a nonnegative measurable function.
%% We define functions $m_h : \R_{\ge 0} \to [0, \mu(S)]$ and $h^* : [0, \mu(S)] \to \R_{\ge 0}$ via
%% \[
%% m_h(t) &= \mu(\{x : h(x) > t\}) \\
%% h^*(u) &= \sup\{t : m_h(t) > u\}
%% \]

%% We can think of $h^*$ as a ``decreasing rearrangement'' of $h$; it is a decreasing measurable function on $[0, \mu(S)]$ with the same distribution as $h$.

\begin{definition}
  Let $f, g : S \to \R_{\ge 0}$ be nonnegative measurable functions such that $\int f\,d\mu = \int g\,d\mu < \infty$.
  We say $g$ \defn{majorizes} $f$ if any of the following equivalent conditions hold:
  \begin{itemize}
  \item $\int \max\{f - t, 0\}\,d\mu \le \int \max\{g - t, 0\}\,d\mu$ for every $t \ge 0$.
  \item $\int_t^\infty m_f(u)\,du \le \int_t^\infty m_g(u)\,du$ for all $t \ge 0$, where $m_h(u) = \mu(\{x : h(x) > u\})$.
  \item $\int_0^r f^*(\ell)\,d\ell \le \int_0^r g^*(\ell)\,d\ell$ for all $0 \le r < \mu(S)$, where $h^*(\ell) = \sup\{t : m_h(t) > \ell\}$.
  \end{itemize}
\end{definition}

We write $f \prec g$ to mean $g$ majorizes $f$.
We prove a technical lemma:

\begin{lemma}\label{lem:attain max}
  Suppose $f \prec g$, and let $b = \max\{\esssup f, \esssup g\}$.  Then $\mu(\{x : f(x) = b\}) \le \mu(\{x : g(x) = b\})$.
\end{lemma}
\begin{proof}
  If $\esssup f \ne \esssup g$ the result is trivial.
  For $h \in \{f, g\}$ and $\epsilon > 0$ we have
  \[
  \int \max\{h - (b - \epsilon), 0\}\,d\mu &\in \epsilon[\mu(h = b), \mu(h \ge b - \epsilon)] \\
  \frac{\int \max\{h - (b - \epsilon), 0\}\,d\mu}{\epsilon} &\in \mu(h = b) + [0, \mu(b - \epsilon \le h < b)] \\
  \lim_{\epsilon \to 0^+} \frac{\int \max\{h - (b - \epsilon), 0\}\,d\mu}{\epsilon} &= \mu(h = b)
  \]
  But $\frac{\int \max\{f - (b - \epsilon), 0\}\,d\mu}{\epsilon} \le \frac{\int \max\{g - (b - \epsilon), 0\}\,d\mu}{\epsilon}$ for all $\epsilon > 0$
  and so $\mu(f = b) \le \mu(g = b)$.
\end{proof}


\begin{theorem}[Theorem 2.1 in Joe 1987]
  \label{thm:majorize}
  Let $\phi$ be a convex function on an interval $T \subset \R$ containing 0
  such that $\phi(0) = 0$.
  Let $f \prec g$ where $f, g$ take values in $T$.
  Then \[\int (\phi \circ f)\,d\mu \le \int (\phi \circ g)\,d\mu\]
  provided that the integrals exist and are finite.
\end{theorem}
\begin{proof}
  First we consider the case where $\phi$ is continuous and $\phi'(0^+) > -\infty$.  In this case, we can approximate $\phi$ by an increasing sequence of finite sums of the form $\phi_n(u) = \phi'(0^+)u + \sum_i a_i \max\{u - b_i, 0\}$ where $a_i, b_i \ge 0$.  Linearity gives us $\int (\phi_n \circ f)\,d\mu \le \int (\phi_n \circ g)\,d\mu$ and the result follows from the monotone convergence theorem.  In this case we don't need the condition that the integrals are finite: they always exist and the result holds even if one or both is $\infty$.

  For the next case we allow $\phi'(0^+) = -\infty$, and assume $\phi$ is continuous at its right endpoint but not necessarily its left.
  We approximate $\phi$ by a sequence of continuous convex functions
  \[\psi_n(u) = 
  \begin{cases}
    n\phi(n^{-1})u & 0 \le u < n^{-1}\\
    \phi(u) & u \ge n^{-1}
  \end{cases}\]
  where each $\psi_n'(0^+) > -\infty$.
  By the first case we have $\int (\psi_n \circ f)\,d\mu \le \int (\psi_n \circ g)\,d\mu$ and we can finish using the dominated convergence theorem since $|\psi_n| \le |\phi|$.

  Finally we handle the case where $\phi$ is discontinuous at its right endpoint, which we call $b$.  We can write $\phi(u) = \widetilde\phi(u) + c[u = b]$ for some $c > 0$ and some $\widetilde\phi$ which satisfies the conditions of the previous case.
  The result then follows from Lemma~\ref{lem:attain max}.
\end{proof}

Note that if $\mu(S) < \infty$ we can work with a general interval $[a, b]$ and dispense with the assumption $\phi(0) = 0$,
by considering the functions \[\phi_a(u) &= \phi(u + a) - \phi(a) \\ f_a(x) &= f(x) - a \\ g_a(x) &= g(x) - a\]
% so we're dropping the non-negativity requirement too? what does majorization mean if they aren't nonnegative? and how does that work with jensen.
% need to be clearer about this so we can get jensen out of it

% i think we can get rid of continuity requirement.  for left end approximating with psi_n already works
% what about for right end?

% can we also generalize to locally-convex topological vector space instead of real-valued?  where does the nonnegativity come in?

% would also need hahn-banach in that case...

From this we can derive familiar results:

\begin{theorem}[Jensen's Inequality]
  Let $X$ be a random variable and $\phi$ a continuous convex function. % xxx shouldnt need phi continuous, also be careful about nonnegativity in majorization...
  Then $\phi(\E X) \le \E[\phi(X)]$.
\end{theorem}
\begin{proof}
  Consider the constant random variable $Y(x) = \E X$.
  Then $Y \prec X$ and so by Theorem~\ref{thm:majorize} we have
  $\phi(\E X) = \E[\phi(Y)] \le \E[\phi(X)]$.
\end{proof}

For random variables with the same mean, $Y \prec X$ intuitively means that $Y$ is more concentrated than $X$; for instance we have $\Var Y \le \Var X$ since $x \mapsto x^2$ is convex.

\section*{Power Means}

Let $X$ be a nonnegative random variable, and define:

\[M_p(X) =
\begin{cases}
  \E[X^p]^{1/p} & p \in \R - \{0\} \\
  \exp \E[\log X] & p = 0 \\
  \essinf(X) & p = -\infty \\
  \esssup(X) & p = \infty \\
\end{cases}\]

\begin{theorem}[Power Mean Inequality]
  If $p \le q$ then $M_p(X) \le M_q(X)$ whenever both exist.
\end{theorem}
\begin{proof}
  If either of $p$ or $q$ is infinite then the result is trivial.

  Suppose $0 = p < q$.  Since $\log$ is concave we have by Jensen's inequality
  \[
  \log \E[X^p] &\ge \E[\log X^p] \\
  \frac1p \log \E[X^p] &\ge \E[\log X] \\
  \E[X^p]^{1/p} &\ge \exp \E[\log X] \\
  \]
  The case where $p < q = 0$ is analogous.

  Now suppose $0 < p < q$.  Since $u \mapsto u^{q/p}$ is convex we have
  \[
  \E[X^p]^{q/p} &\le \E[X^q] \\
  \E[X^p]^{1/p} &\le \E[X^q]^{1/q} \\
  \]
  The case where $p < q < 0$ is analogous.
\end{proof}

% xxx need to prove existence of M_0 so there's no issue with transitivity
% also maybe clarify 1/0 = +inf in this context

For instance this generalizes the AM-GM inequality since $M_0$ is the geometric mean and $M_1$ is the arithmetic mean.

\section*{H\"older's Inequality}

Let $f : S \to \R$ be a nonnegative measurable function.
Define \[\norm{f}_p = \begin{cases}
  \left(\int f\,d\mu\right)^{1/p} & 1 \le p < \infty \\
  \esssup f & p = \infty \\
  \end{cases}
\]
Note that the results from the previous section do not directly apply here since $S$ is not necessarily a probability space.  Nonetheless we can make use of them by defining an appropriate probability measure.

\begin{theorem}[H\"older's Inequality]
  Let $f, g : S \to \R$ be nonnegative measurable functions,
  and let $p, q \in [1, \infty]$ such that $\frac1p + \frac1q = 1$.
  Then $\norm{fg}_1 \le \norm{f}_p \norm{g}_q$.
\end{theorem}
\begin{proof}
  The case where $p$ or $q$ is $\infty$ is trivial, so we skip it.
  Define $\nu$ to be a probability measure on $S$ via $\nu(A) = \frac{\int_A g^q\,d\mu}{\int g^q\,d\mu}$,
  and let $X = fg^{1-q}$.  (We can assume $\int g^q\,d\mu \in (0, \infty)$ since otherwise the result is trivial.)
  Then we have
  \[\norm{fg}_1 &= \left(\int g^q\,d\mu\right)\left(\int f g^{1-q} \frac{g^q}{\int g^q\,d\mu}\,d\mu\right) \\
  &= \left(\int g^q\,d\mu\right)\E_\nu[X] \\
  &\le \left(\int g^q\,d\mu\right)\E_\nu[X^p]^{1/p} \\
  &= \left(\int g^q\,d\mu\right)\left(\int f^p g^{p(1-q)}\frac{g^q}{\int g^q\,d\mu}\,d\mu \right)^{1/p} \\
  &= \left(\int g^q\,d\mu\right)^{1 - 1/p}\left(\int f^p g^{p + q - pq}\,d\mu \right)^{1/p} \\
  &= \norm{f}_p \norm{g}_q \\
\]
\end{proof}

\end{document}
