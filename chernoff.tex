\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{latexsym}
\usepackage{gensymb}
\usepackage{mathtools}
\usepackage[margin=1in]{geometry}
\usepackage[shortlabels]{enumitem}

\usepackage{libertine}
\usepackage[libertine,vvarbb]{newtxmath}
\usepackage[scaled=0.96]{zi4}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\usepackage{xcolor}
\usepackage[colorlinks, linkcolor=purple, citecolor=blue!75!black, urlcolor=purple]{hyperref}

\usepackage{tcolorbox}

\usepackage[noabbrev,capitalise]{cleveref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\newcommand*{\defn}[1]{\textbf{#1}}
\newcommand*{\N}[0]{\mathbb{N}}
\newcommand*{\R}[0]{\mathbb{R}}
\newcommand*{\E}[0]{\mathbf{E}}
\DeclareMathOperator*{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand*{\Normal}[0]{\mathcal{N}}
\newcommand*{\Exp}[0]{\text{Exp}}
\newcommand*{\Fl}[0]{\mathcal{F}}
\newcommand*{\tp}[0]{\mathsf{T}}

\def\[#1\]{\begin{align*}#1\end{align*}}
\newcommand*{\pheq}{\mathrel{\phantom{=}}} % invisible = for align mode
\newcommand*{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand*{\iff}[0]{\leftrightarrow}

\title{Concentration Bounds for Randomized Algorithms}
\author{Lily Chung}
\date{}

\begin{document}
\maketitle

Here I collect some simplified concentration inequalities for bounding quantities which commonly appear in randomized algorithms.
These bounds are quite loose since the only goal is to obtain asymptotic results which can be applied quickly and easily.

\section{High Probability Bounds}

\begin{definition}
  Let \(A_{n, c}\) be an event depending on \(n\) and a value \(c\).
  We say \(A\) holds \defn{with high probability} if
  for any choice of \(c\),
  \(Pr[\overline{A_{n, c}}] \in O(n^{-c})\).\footnote{This notion of ``w.h.p.'' in algorithms differs from other fields which use it to mean that $\Pr[\overline{A_n}] \in o(1)$.}
\end{definition}

In particular we might have a random variable \(X(n)\) depending on \(n\).
If we say, ``\(X\) is \(O(f)\) with high probability'',
we mean that for any constant \(c\), there exist constants \(a, b, N\) such that
\(\Pr[X(n) > af(n)] < bn^{-c}\) for all \(n > N\).
In other words, the constants \(a, b, N\) hidden by the big-O notation are allowed to depend on \(c\).

In particular, suppose \(p\) is a polynomial and we have \(p(n)\) random variables \(X_i\),
such that each \(X_i\) is \(O(f)\) with high probability.
Then by a union bound, \(\max_i X_i\) is \(O(f)\) with high probability.

\section{Applying Chernoff-Hoeffding Bounds}

% all of this is wrong because i copied down the hoeffding bound wrong sigh.  redo it later

%% \begin{theorem}[Hoeffding]
%%   \label{thm:hoeffding}
%%   Let $X = \sum_i X_i$ where the $X_i$ are independent and $X_i \in [0, 1]$.
%%   Then \[\Pr[X - \E X \ge t] \le \exp(-2t^2)\]
%%   for all $t \ge 0$.
%% \end{theorem}

%% Note that the \(X_i\) need not be identically distributed, nor are they required to take only the values \(\{0, 1\}\).
%% A simple consequence is the following:

%% \begin{tcolorbox}
%%   \begin{corollary}
%%     Let \(X = \sum_i X_i\) where the \(X_i\) are independent and \(X \in [0, 1]\).
%%     \begin{itemize}
%%     \item If \(\E X \in O(\sqrt{\log n})\), then with high probability \(X \in O(\sqrt{\log n})\).
%%     \item If \(\E X \in \Omega(\sqrt{\log n})\), then with high probability \(X \in O(\E X)\).
%%     \end{itemize}
%%   \end{corollary}
%% \end{tcolorbox}
%% \begin{proof}\hfill
%%   \begin{itemize}
%%   \item
%%     Let \(t = c \sqrt{\log n}\) for any \(c\) large enough so that eventually \(\E X \le t\).
%%     Then \[\Pr[X \ge 2c \sqrt{\log n}] \le \exp(-2c^2 \log n) = n^{-2c^2}\] by \cref{thm:hoeffding}.
%%   \item
%%     Pick any \(c\),
%%     and let \(d \ge 1\) be a constant large enough so that eventually \(d \E X \ge c \sqrt{\log n}\).
%%     Applying \cref{thm:hoeffding},
%%     we find that \[\Pr[X \ge (d + 1)\E X] \le \exp(-2(d \E X)^2) \le \exp(-2c^2\log n) = n^{-2c^2}\]
%%   \end{itemize}
%% \end{proof}

% todo amplifying distinguisher (biased coin)

\section{Bounded Independence}
% todo revise section

The Chernoff bound requires that the variables \(X_i\) are fully independent.
In some contexts it can be useful to reduce the amount of independence, and thus the number of random bits needed to generate the variables.
It turns out we still have useful concentration bounds even when the variables are only \(k\)-wise independent.

\begin{theorem}[Theorem 5(IIa) of \cite{limited-chernoff}]
  \label{thm:limited-chernoff}
  Let \(X = \sum_i X_i\) such that the \(X_i\) are \(k\)-wise independent and confined to the range \([0, 1]\) with \(\mu = E[X]\).
  Let \(\delta \ge 1\) and suppose \(k \le \lfloor \delta \mu e^{-1/3} \rfloor.\)
  Then \[\Pr[X \ge (1 + \delta)\mu] \le \exp(-\lfloor k/2 \rfloor).\]
\end{theorem}

Again we'll put this into an easier form to apply:

\begin{tcolorbox}
  \begin{corollary}
    \label{cor:limited-chernoff-simple}
    Let \(X = \sum_i X_i\) such that the \(X_i\) are \(k\)-wise independent and confined to the range \([0, 1]\) with \(\mu \ge E[X]\).
    Suppose \(k \le \mu.\)
    Then \[\Pr[X \ge 4\mu] \le \exp(-\lfloor k/2 \rfloor).\]
  \end{corollary}
\end{tcolorbox}
\begin{proof}
  Without loss of generality assume \(\mu \ge 2\) since otherwise the statement is trivial.
  Let \(Y = X + \mu - E[X]\) as before; then \(Y\) is a sum of \(k\)-wise independent \([0, 1]\) random variables with \(\mu = E[Y]\).
  Set \(\delta = 3\) so that \(k \le \mu \le \lfloor 2\mu \rfloor \le \lfloor \delta \mu e^{-1/3} \rfloor\).
  Applying Theorem~\ref{thm:limited-chernoff} to \(Y\), we obtain
  \(\Pr[X \ge 4\mu] \le \Pr[Y \ge 4\mu] \le \exp(-\lfloor k/2 \rfloor).\)
\end{proof}

We can now show similar results to before but with only \(\Theta(\log n)\) independent bits.

\begin{tcolorbox}
  \begin{corollary}
    Let \(X = \sum_i X_i\) such that the \(X_i\) are \(k\)-wise independent and confined to the range \([0, 1]\), where \(k = 2\lceil c\log n \rceil\).
    \begin{itemize}
    \item If \(E[X] \in O(\log n)\), then  \(X \in O(\log n)\) with probability \(1 - n^{-c}\).
    \item If \(E[X] \in \Omega(\log n)\), then \(X \in O(E[X])\) with probability \(1 - n^{-c}\).
    \end{itemize}
  \end{corollary}
\end{tcolorbox}
\begin{proof}\hfill
  \begin{itemize}
  \item
    Let \(d \ge k\) be a constant large enough so that eventually \(E[X] \le d \log n\).
    Apply Corollary~\ref{cor:limited-chernoff-simple} with \(\mu = d \log n\); 
    then eventually \(\Pr[X \ge 4d \log n] \le \exp(-\lfloor k/2 \rfloor) \le n^{-c}\).
  \item
    Let \(d\) be a constant large enough so that eventually \(d E[X] \ge k\).
    Apply Corollary~\ref{cor:limited-chernoff-simple} with \(\mu = dE[X]\);
    then eventually \(\Pr[X \ge 4dE[X]] \le \exp(-\lfloor k/2 \rfloor) \le n^{-c}\).
  \end{itemize}
\end{proof}

\end{document}
