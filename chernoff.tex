 \documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\input{liatex/preamble}

\usepackage{tcolorbox}

\title{Applying Concentration}
\author{Lily Chung}
\date{}

\begin{document}
\maketitle

We discuss some common concentration inequalities with a view towards easily conceptualizing and applying them.
For most applications in computer science we don't need to be concerned about the exact numerical constants,
so we will ignore constant factors.  Instances of asymptotic notation ($O, \Theta$, etc.) are with respect to the inverse error probability $\delta^{-1}$.

One common scenario is that we have a sequence of independent $[0, 1]$ random variables,
and we are interested in their sum.  The following theorems are often used to understand the deviations of this sum from its mean.

\begin{theorem}[Hoeffding]
  \label{thm:hoeffding}
  Let $X = \sum_{i=1}^k X_i$ where the $X_i$ are independent and $X_i \in [0, 1]$.
  Then \[\Pr[X - \E X \ge t] &\le \exp(-2t^2/k) & t \ge 0\]
\end{theorem}

\begin{theorem}[Chernoff]
  \label{thm:chernoff}
  Let \(X = \sum_i X_i\) where the \(X_i\) are independent and $X_i \in [0, 1]$, and let \(\mu = \E X\).
  Then \[
  \Pr[X \ge (1 + \delta)\mu] &\le \exp\left(-\frac{b^2}{2+b}\mu\right) & b \ge 0 \\
  \Pr[X \le (1 - \delta)\mu] &\le \exp\left(-\frac{b^2}{2}\mu\right) & b \in (0, 1) \\
  \]
\end{theorem}

How can we understand what these theorems are saying in practice?
One way is to fix a small ``error'' probability $\delta$,
and ask how large the fluctuations of $X$ are while ignoring an exceptional event of probability $\le \delta$.
If we invert \cref{thm:hoeffding} in this way, we find that with probability $1 - \delta$,
\[X = \E X \pm \sqrt{\frac12 k \log(2\delta^{-1})} = \E X \pm O\left(\sqrt{k \log \delta^{-1}}\right)\]

How should we understand the quantity $\log \delta^{-1}$ occurring in the above?  One consideration is that we often are not interested in just one of these bounds in isolation, but rather in the behavior of a large system.  For instance, suppose $n$ is some parameter, and we have a polynomial number $n^c$ of random variables distributed as $X$ (not necessarily independent).
If we wish to control the fluctuations of all of these variables at once with only $1\%$ of error probability, we can take a union bound, which $\delta = .01 n^{-c}$.
In this case we learn that with $99\%$ probability, all of our $n^c$ variables lie in the range
\[\E X \pm O\left(\sqrt{k \log n}\right)\]
where the constant in the $O$ depends only on $c$.

We can similarly analyze \cref{thm:chernoff}.
An elementary observation which is useful in practice is that even without knowing $\mu$ exactly, we can still apply the Chernoff bound using an upper or lower bound on $\mu$:

\begin{corollary}
  \label{cor:chernoff-ineq}
  Let \(X = \sum_i X_i\) such that the \(X_i\) are independent and $X_i \in [0, 1]$,
  and suppose $\ell \le \E X \le u$.
  Then \[
  \Pr[X \ge (1 + \delta)u] &\le \exp\left(-\frac{b^2}{2+b}u\right) & b \ge 0 \\
  \Pr[X \le (1 - \delta)\ell] &\le \exp\left(-\frac{b^2}{2}\ell\right) & b \in (0, 1) \\
  \]
\end{corollary}
\begin{proof}
  The lower bound in terms of $\ell$ is trivial since all the inequalities are in the correct direction,
  but the upper bound is a bit more difficult because $\exp(-u) \le \exp(-\E X)$.
  To prove it, let \(Y = X + u - \E X\), so that $\E Y = u$ and $X \le Y$.
  Then \(Y\) can be expressed as a sum of independent $[0, 1]$ random variables,
  and applying \cref{thm:chernoff} to \(Y\) gives the result.
\end{proof}

The results of inverting these theorems and dropping constant factors are as follows: 

\begin{tcolorbox}
  \begin{lemma}
    Let \(X = \sum_{i=1}^{k(n)} X_i\) where the \(X_i\) are independent and \(X \in [0, 1]\).  Then the following hold with probability $1 - \delta$:
    \begin{itemize}
    \item $X = \E X \pm O(\sqrt{k(n) \log \delta^{-1}})$.
    \item $X \in O(\E X + \log \delta^{-1})$.
    \item If $\E X \in \omega(\log \delta^{-1})$, then $X \in \Theta(\E X)$.
    \end{itemize}
  \end{lemma}
\end{tcolorbox}
\begin{proof}\hfill
  \begin{itemize}
  \item
    Union bound the upper and lower tails of \cref{thm:hoeffding} together.
  \item
    Set $u = \max\{\E X, \log \delta^{-1}\}$.
    Then we have
    \[\Pr[X \ge 3u] \le \exp(-u) \le \exp(-\log \delta^{-1}) = \delta\]
    by \cref{cor:chernoff-ineq}.
  \item
    Eventually $\E X \ge 8 \log \delta^{-1}$, so
    \[\Pr[X \le \E X / 2] \le \exp(-\E X / 8) \le \exp(-\log \delta^{-1}) = \delta\]
    by \cref{thm:chernoff}.
  \end{itemize}
\end{proof}

Some more applications:

\begin{itemize}
\item
  Suppose we have a biased coin which flips heads with an unknown probability $p$.
  We wish to estimate $p$ by averaging the results of $k$ flips.
  Then by \cref{thm:hoeffding} we should take $k = \Theta(\epsilon^{-2} \log \delta^{-1})$ flips
  to get an additive estimate $p \pm \epsilon$ with probability $1 - \delta$.
\item
  Suppose we have an algorithm which distinguishes distributions $P, Q$ with advantage $\epsilon$:
  \[
  \left|\Pr_{X \sim P}[A(X) = 1] - \Pr_{X \sim Q}[A(X) = 1]\right| \ge \epsilon
  \]
  What if we wish to amplify this distinguisher so that we are very likely to obtain the correct answer?
  We can accomplish this by taking $k$ independent samples from the unknown distribution,
  running $A$ on each of them, and determining whether the average output is closer to $\Pr_{X \sim P}[A(X) = 1]$ or to $\Pr_{X \sim Q}[A(X) = 1]$.
  We need to take enough samples so that our estimate is within $\epsilon/2$ of the true answer.
  So this is just the previous application in disguise, and we should again take $k = \Theta(\epsilon^{-2} \log \delta^{-1})$
  to ensure we get the correct answer with probability $1 - \delta$.
\end{itemize}

\section{Bounded Independence}
% todo revise section

The Chernoff bound requires that the variables \(X_i\) are fully independent.
In some contexts it can be useful to reduce the amount of independence, and thus the number of random bits needed to generate the variables.
It turns out we still have useful concentration bounds even when the variables are only \(k\)-wise independent.

\begin{theorem}[Theorem 5(IIa) of \cite{limited-chernoff}]
  \label{thm:limited-chernoff}
  Let \(X = \sum_i X_i\) such that the \(X_i\) are \(k\)-wise independent and confined to the range \([0, 1]\) with \(\mu = \E X\).
  Let \(\delta \ge 1\) and suppose \(k \le \lfloor \delta \mu e^{-1/3} \rfloor.\)
  Then \[\Pr[X \ge (1 + \delta)\mu] \le \exp(-\lfloor k/2 \rfloor).\]
\end{theorem}

Again we'll put this into an easier form to apply:

\begin{tcolorbox}
  \begin{corollary}
    \label{cor:limited-chernoff-simple}
    Let \(X = \sum_i X_i\) such that the \(X_i\) are \(k\)-wise independent and confined to the range \([0, 1]\) with \(\mu \ge \E X\).
    Suppose \(k \le \mu.\)
    Then \[\Pr[X \ge 4\mu] \le \exp(-\lfloor k/2 \rfloor).\]
  \end{corollary}
\end{tcolorbox}
\begin{proof}
  Without loss of generality assume \(\mu \ge 2\) since otherwise the statement is trivial.
  Let \(Y = X + \mu - \E X\) as before; then \(Y\) is a sum of \(k\)-wise independent \([0, 1]\) random variables with \(\mu = E[Y]\).
  Set \(\delta = 3\) so that \(k \le \mu \le \lfloor 2\mu \rfloor \le \lfloor \delta \mu e^{-1/3} \rfloor\).
  Applying Theorem~\ref{thm:limited-chernoff} to \(Y\), we obtain
  \(\Pr[X \ge 4\mu] \le \Pr[Y \ge 4\mu] \le \exp(-\lfloor k/2 \rfloor).\)
\end{proof}

We can now show similar results to before but with only \(\Theta(\log n)\) independent bits.

\begin{tcolorbox}
  \begin{corollary}
    Let \(X = \sum_i X_i\) such that the \(X_i\) are \(k\)-wise independent and confined to the range \([0, 1]\), where \(k = 2\lceil c\log n \rceil\).
    \begin{itemize}
    \item If \(\E X \in O(\log n)\), then  \(X \in O(\log n)\) with probability \(1 - n^{-c}\).
    \item If \(\E X \in \Omega(\log n)\), then \(X \in O(\E X)\) with probability \(1 - n^{-c}\).
    \end{itemize}
  \end{corollary}
\end{tcolorbox}
\begin{proof}\hfill
  \begin{itemize}
  \item
    Let \(d \ge k\) be a constant large enough so that eventually \(\E X \le d \log n\).
    Apply Corollary~\ref{cor:limited-chernoff-simple} with \(\mu = d \log n\); 
    then eventually \(\Pr[X \ge 4d \log n] \le \exp(-\lfloor k/2 \rfloor) \le n^{-c}\).
  \item
    Let \(d\) be a constant large enough so that eventually \(d \E X \ge k\).
    Apply Corollary~\ref{cor:limited-chernoff-simple} with \(\mu = d\E X\);
    then eventually \(\Pr[X \ge 4d\E X] \le \exp(-\lfloor k/2 \rfloor) \le n^{-c}\).
  \end{itemize}
\end{proof}

\end{document}
